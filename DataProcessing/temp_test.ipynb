{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b4a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3222347522\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import ipaddress\n",
    "\n",
    "# --- 1. Convert IP addresses to float64 ---\n",
    "def ip_to_float(ip):\n",
    "    try:\n",
    "        return float(int(ipaddress.IPv4Address(ip)))/1e9\n",
    "    except:\n",
    "        return 0.0  # for invalid or empty IPs\n",
    "    \n",
    "print(ip_to_float(\"192.17.31.2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df[\"label\"].unique().compute()\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c8a4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   proto_tcp  proto_udp\n",
      "0          1          0\n",
      "1          0          1\n",
      "2          1          0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'proto': ['tcp', 'udp', 'tcp']\n",
    "})\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=['proto'], prefix=['proto'], dtype=int)\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03dc0af",
   "metadata": {},
   "source": [
    "> 0                           C&C-Mirai\n",
    "0                    C&C-FileDownload\n",
    "0          C&C-HeartBeat-FileDownload\n",
    "0           PartOfAHorizontalPortScan\n",
    "0                        Okiru-Attack\n",
    "0                        FileDownload\n",
    "0                                 C&C\n",
    "0                           C&C-Torii\n",
    "0                               Okiru\n",
    "0                              Attack\n",
    "0    PartOfAHorizontalPortScan-Attack\n",
    "0                                DDoS\n",
    "0       C&C-PartOfAHorizontalPortScan\n",
    "0                C&C-HeartBeat-Attack\n",
    "0                       C&C-HeartBeat\n",
    "0                                   -\n",
    "Name: detailed-label, dtype: string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b5f75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m attack_types = [\u001b[43mC\u001b[49m&C-Mirai, C&C-FileDownload, C&C-HeartBeat-FileDownload, PartOfAHorizontalPortScan, Okiru-Attack, FileDownload, C&C, \\\n\u001b[32m      2\u001b[39m                 C&C-Torii, Okiru, Attack, PartOfAHorizontalPortScan-Attack, DDoS, C&C-PartOfAHorizontalPortScan, C&C-HeartBeat-Attack, C&C-HeartBeat, \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(attack_types))\n",
      "\u001b[31mNameError\u001b[39m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "attack_types = [C&C-Mirai, C&C-FileDownload, C&C-HeartBeat-FileDownload, PartOfAHorizontalPortScan, Okiru-Attack, FileDownload, C&C, /\n",
    "                C&C-Torii, Okiru, Attack, PartOfAHorizontalPortScan-Attack, DDoS, C&C-PartOfAHorizontalPortScan, C&C-HeartBeat-Attack, C&C-HeartBeat, \"-\"]\n",
    "\n",
    "print(len(attack_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf03865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.014545\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "start =datetime.now()\n",
    "def string_to_float(s):\n",
    "    return int(hashlib.sha256(s.encode('utf-8')).hexdigest(), 16) % 10**6 / 1e6\n",
    "\n",
    "for index in range(10000):\n",
    "    temp = string_to_float(\"ShA\")\n",
    "end = datetime.now()\n",
    "\n",
    "time = str((end - start))\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1cc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipeline: Standardize -> Normalize\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('normalizer', Normalizer(norm='l2'))\n",
    "])\n",
    "\n",
    "df_processed = pd.DataFrame(pipeline.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"/nStandardized + L2 Normalized Data:\")\n",
    "print(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ada035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from dask.dataframe import isna\n",
    "# from dask.dataframe import notna\n",
    "# Tạo DataFrame mẫu\n",
    "df = dd.from_pandas(pd.DataFrame({\n",
    "    'A': [1, np.nan, 3],\n",
    "    'B': [np.nan, np.nan, np.nan],\n",
    "    'C': ['x', 'y', np.nan]\n",
    "}), npartitions=2)\n",
    "\n",
    "# Lọc cột có dữ liệu\n",
    "non_empty_cols = df.columns[(~isna(df)).any().compute()]\n",
    "df = df[non_empty_cols]\n",
    "print(df.shape[0].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a1189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "    col1      col2  col3\n",
      "0     1  0.200000     5\n",
      "1     2  0.212131     6\n",
      "2     3  0.100000     7\n",
      "3     4  0.600000     8\n",
      "col1    category\n",
      "col2     float64\n",
      "col3    category\n",
      "dtype: object\n",
      "After Scaler: \n",
      "        col2  proto_1  proto_2  proto_3  proto_4  service_5  service_6  \\\n",
      "0 -0.408736        1        0        0        0          1          0   \n",
      "1 -0.345192        0        1        0        0          0          1   \n",
      "2 -0.932535        0        0        1        0          0          0   \n",
      "3  1.686463        0        0        0        1          0          0   \n",
      "\n",
      "   service_7  service_8  \n",
      "0          0          0  \n",
      "1          0          0  \n",
      "2          1          0  \n",
      "3          0          1  \n",
      "After normal\n",
      "        col2   proto_1  proto_2  proto_3  proto_4  service_5  service_6  \\\n",
      "0 -0.378351  0.925662        0        0        0          1          0   \n",
      "1 -1.000000  0.000000        1        0        0          0          1   \n",
      "2 -1.000000  0.000000        0        1        0          0          0   \n",
      "3  1.000000  0.000000        0        0        1          0          0   \n",
      "\n",
      "   service_7  service_8  \n",
      "0          0          0  \n",
      "1          0          0  \n",
      "2          1          0  \n",
      "3          0          1  \n",
      "Solving  0\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from dask_ml.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from dask.dataframe import isna\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "\n",
    "# Tạo DataFrame Dask từ Pandas DataFrame\n",
    "df = dd.from_pandas(pd.DataFrame({\n",
    "    'col1': [1, 2, 3, 4],\n",
    "    'col2': [0.2, 0.2121314, 0.1 ,0.6],\n",
    "    'col3': [5,6,7,8],\n",
    "    # 'col4': [np.nan, np.nan, np.nan, np.nan]\n",
    "}), npartitions=1)\n",
    "\n",
    "\n",
    "df = df.replace([\"(empty)\", \"-\"], np.nan)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(\"Before: /n\",df.compute())\n",
    "\n",
    "\n",
    "df = df.categorize(columns=['col1', 'col3'])\n",
    "print(df.dtypes)\n",
    "df= dd.get_dummies(\n",
    "    df, \n",
    "    columns=['col1', 'col3'],\n",
    "    prefix={'col1': 'proto', 'col3':'service'},  # Custom prefix\n",
    "    # dummy_na=True,  # Tạo cột cho giá trị NaN\n",
    "    # drop_first=True,  # Bỏ đi một cột để tránh đa cộng tuyến\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "\n",
    "df = df.fillna(\"0\")\n",
    "\n",
    "numeric_cols = ['col2']\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "scaler = StandardScaler()\n",
    "def Standard_Scaling(partition):\n",
    "    partition[numeric_cols] = scaler.fit_transform(partition[numeric_cols])\n",
    "    return partition\n",
    "df = df.map_partitions(Standard_Scaling, meta= dict(df.dtypes))\n",
    "print(\"After Scaler: /n\", df.compute())\n",
    "\n",
    "# def normalize_partition(partition):\n",
    "#     normalizer = Normalizer(norm='l2')\n",
    "#     partition[numeric_cols] = normalizer.fit_transform(partition[numeric_cols])\n",
    "#     print(partition)\n",
    "#     return partition\n",
    "\n",
    "# print(dict(df.dtypes))\n",
    "\n",
    "# df = df.map_partitions(normalize_partition, meta=dict(df.dtypes))\n",
    "\n",
    "    # def sum_of_squares(partition):\n",
    "    #     return pd.Series([(partition ** 2).sum()])\n",
    "\n",
    "    # Tính tổng bình phương theo từng partition, rồi cộng lại\n",
    "    # sum_squares = df['col2'].map_partitions(sum_of_squares).sum().compute()\n",
    "    # l2_norm = np.sqrt(sum_squares)\n",
    "    # df['col2'] = df['col2'] / l2_norm\n",
    "\n",
    "def normalize_partition(pdf):\n",
    "    row_norm = np.sqrt((pdf**2).sum(axis=1))\n",
    "    row_norm[row_norm == 0] = np.nan\n",
    "    return pdf.div(row_norm, axis=0)\n",
    "cols = ['col2', 'proto_1']\n",
    "df[cols] = df[cols].map_partitions(normalize_partition, meta=dict(df[cols].dtypes))\n",
    "\n",
    "def col_is_all_nan(df, col):\n",
    "    return df[col].isna().all()\n",
    "\n",
    "# print(col_is_all_nan(df, 'col2'))\n",
    "# # Đánh giá từng cột, từng partition\n",
    "# results = []\n",
    "# for col in df.columns:\n",
    "#     # Tính isna().all() trên từng partition\n",
    "#     parts = df.map_partitions(col_is_all_nan, col=col, meta={col: 'bool'}).compute()\n",
    "    \n",
    "#     # Nếu tất cả partition trả về True => cột này toàn NaN\n",
    "#     if parts.all():\n",
    "#         results.append(col)\n",
    "\n",
    "# # Drop các cột rỗng toàn phần\n",
    "# df = df.drop(columns=results)\n",
    "\n",
    "print(\"After normal/n\", df.compute())\n",
    "\n",
    "output_file =\"temp.csv\"\n",
    "\n",
    "\n",
    "header_saved= False\n",
    "for i, partition in enumerate(df.partitions):\n",
    "    print(\"Solving \", i)\n",
    "    partition.compute().to_csv(output_file, index=False, mode=\"a\", header= not  header_saved)  # Append không ghi header\n",
    "    header_saved=True\n",
    "# Tính toán và xem kết quả\n",
    "\n",
    "# df = dd.read_csv('temp.csv')\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0760b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "s = \"s\"\n",
    "print(type(str(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c38c6c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x   y   z\n",
      "0  0  10  10\n",
      "1  1  11  12\n",
      "2  2  12  14\n",
      "3  3  13  16\n",
      "4  4  14  18\n",
      "5  5  15  20\n",
      "6  6  16  22\n",
      "7  7  17  24\n",
      "8  8  18  26\n",
      "9  9  19  28\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# Tạo Dask DataFrame từ Pandas\n",
    "pdf = pd.DataFrame({'x': range(10), 'y': range(10, 20)})\n",
    "ddf = dd.from_pandas(pdf, npartitions=2)\n",
    "\n",
    "# Hàm xử lý từng partition (hàm nhận và trả về Pandas DataFrame)\n",
    "def custom_func(df):\n",
    "    df['z'] = df['x'] + df['y']\n",
    "    return df\n",
    "\n",
    "# Áp dụng hàm qua từng partition\n",
    "ddf_new = ddf.map_partitions(custom_func)\n",
    "\n",
    "# Thực thi và in kết quả\n",
    "print(ddf_new.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e38d8381",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+----------+--------+----------+\n| Column   | Found  | Expected |\n+----------+--------+----------+\n| duration | object | float64  |\n+----------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- duration\n  ValueError(\"could not convert string to float: '-'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'duration': 'object'}\n\nto the call to `read_csv`/`read_table`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     31\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mastype({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mduration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# ##### FUNC ############\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# def ip_to_float(ip):\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#     if index == 10:\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#         break\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask_expr/_collection.py:477\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[0;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    476\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask/base.py:376\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask/base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask_expr/_expr.py:3758\u001b[0m, in \u001b[0;36mFused._execute_task\u001b[0;34m(graph, name, *deps)\u001b[0m\n\u001b[1;32m   3756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(deps):\n\u001b[1;32m   3757\u001b[0m     graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m dep\n\u001b[0;32m-> 3758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask/dataframe/io/csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[0;34m(self, part)\u001b[0m\n\u001b[1;32m    139\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_read_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrest_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask/dataframe/io/csv.py:197\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[0;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[1;32m    195\u001b[0m df \u001b[38;5;241m=\u001b[39m reader(bio, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[0;32m--> 197\u001b[0m     \u001b[43mcoerce_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns, columns)\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask/dataframe/io/csv.py:298\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[0;34m(df, dtypes)\u001b[0m\n\u001b[1;32m    294\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m61\u001b[39m)\n\u001b[1;32m    295\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    296\u001b[0m     rule\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[1;32m    297\u001b[0m )\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+----------+--------+----------+\n| Column   | Found  | Expected |\n+----------+--------+----------+\n| duration | object | float64  |\n+----------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- duration\n  ValueError(\"could not convert string to float: '-'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'duration': 'object'}\n\nto the call to `read_csv`/`read_table`."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import hashlib\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import ipaddress\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Khởi tạo scaler\n",
    "scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Cấu hình\n",
    "chunk_size = 1000000\n",
    "input_file = \"/mnt/c/Users/hoang/FileCSV_DACN_2025/bigTemp.csv\"\n",
    "output_file = \"C:/Users/hoang/FileCSV_DACN_2025/iot23_cleaned.csv\"\n",
    "\n",
    "cols_to_drop = ['uid', 'ts', 'tunnel_parents', 'local_orig', 'local_resp']\n",
    "ip_to_float_Cols = ['id.orig_h', 'id.resp_h']\n",
    "str_to_float_Cols = ['conn_state', 'history', 'id.resp_p', 'id.orig_p']\n",
    "numeric_cols = ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes']\n",
    "onehot_cols = ['proto', 'service']\n",
    "\n",
    "label_mapping = {\"benign\": 0, \"Benign\": 0, \"Malicious\": 1, \"malicious\": 1}\n",
    "\n",
    "df = dd.read_csv(input_file)\n",
    "df = df.replace([ \"-\", '-'], np.nan)\n",
    "df = df.fillna(0)\n",
    "df = df.astype({'duration': 'float32'})\n",
    "print(df['duration'].unique().compute())\n",
    "\n",
    "\n",
    "# ##### FUNC ############\n",
    "# def ip_to_float(ip):\n",
    "#     try:\n",
    "#         return float(int(ipaddress.IPv4Address(ip)))/1e9\n",
    "#     except:\n",
    "#         return 0.0  # for invalid or empty IPs\n",
    "    \n",
    "# def sum_of_squares(partition):\n",
    "#     return pd.Series([(partition ** 2).sum()])\n",
    "\n",
    "# def string_to_float(s):\n",
    "#     if pd.notna(s):\n",
    "#         return int(hashlib.sha256(str(s).encode('utf-8')).hexdigest(), 16) % 10**8 / 1e8\n",
    "#     return 0\n",
    "# ##### FUNC ##############\n",
    "\n",
    "# proto_categories =set()\n",
    "# service_categories = set()\n",
    "\n",
    "# for index, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size, dtype='str')):\n",
    "#     proto_categories.update(chunk['proto'].unique())\n",
    "#     service_categories.update(chunk['service'].unique())\n",
    "#     print(index)\n",
    "    \n",
    "# for index, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size, dtype='str')):\n",
    "#     chunk['proto'] = pd.Categorical(chunk['proto'], categories=proto_categories)\n",
    "#     chunk['service'] = pd.Categorical(chunk['service'], categories=service_categories)\n",
    "    \n",
    "#     chunk = pd.get_dummies(chunk,columns=onehot_cols, prefix =onehot_cols, dtype='int32')\n",
    "#     print(chunk.describe)\n",
    "    \n",
    "#     if index == 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac692aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conn_state\n",
      "S2              267\n",
      "OTH         3715156\n",
      "SH              380\n",
      "RSTRH          1341\n",
      "S1              220\n",
      "SHR              62\n",
      "RSTR           2282\n",
      "SF            52281\n",
      "S0        319354204\n",
      "RSTO           3712\n",
      "S3             2485\n",
      "REJ           51509\n",
      "RSTOS0      2126047\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['conn_state'].value_counts().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d4329e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_pkts\n",
      "154        9\n",
      "24       342\n",
      "46       445\n",
      "234     1082\n",
      "171       43\n",
      "        ... \n",
      "5100       1\n",
      "265      253\n",
      "328        1\n",
      "205      781\n",
      "8295       1\n",
      "Name: count, Length: 381, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['orig_pkts'].value_counts().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a55aa4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '-'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32mparsers.pyx:1161\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('O') to dtype('float32') according to the rule 'safe'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mduration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask_expr/_collection.py:477\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[0;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    476\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask/base.py:376\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask/base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask_expr/_expr.py:3758\u001b[0m, in \u001b[0;36mFused._execute_task\u001b[0;34m(graph, name, *deps)\u001b[0m\n\u001b[1;32m   3756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(deps):\n\u001b[1;32m   3757\u001b[0m     graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m dep\n\u001b[0;32m-> 3758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask/dataframe/io/csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[0;34m(self, part)\u001b[0m\n\u001b[1;32m    139\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_read_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrest_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask/dataframe/io/csv.py:195\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[0;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[1;32m    193\u001b[0m bio\u001b[38;5;241m.\u001b[39mwrite(b)\n\u001b[1;32m    194\u001b[0m bio\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 195\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[1;32m    197\u001b[0m     coerce_dtypes(df, dtypes)\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1066\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1167\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '-'"
     ]
    }
   ],
   "source": [
    "print(df['duration'].value_counts().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0dafd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.9)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c308bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/hoang/FileCSV_DACN_2025/fl_IoT23/parquet_f0', 'C:/Users/hoang/FileCSV_DACN_2025/fl_IoT23/parquet_f1', 'C:/Users/hoang/FileCSV_DACN_2025/fl_IoT23/parquet_f2']\n",
      "Tỷ lệ file Dask DataFrame Structure:\n",
      "              id.orig_h id.orig_p id.resp_h id.resp_p duration orig_bytes resp_bytes conn_state missed_bytes  history orig_pkts orig_ip_bytes resp_pkts resp_ip_bytes  label detailed-label proto_icmp proto_tcp proto_udp service_dhcp service_dns service_http service_irc service_ssh service_ssl\n",
      "npartitions=8                                                                                                                                                                                                                                                                                       \n",
      "                float64   float64   float64   float64  float64    float64    float64    float64      float64  float64   float64       float64   float64       float64  int64          int64      int64     int64     int64        int64       int64        int64       int64       int64       int64\n",
      "                    ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "...                 ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "                    ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "                    ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "Dask Name: read_parquet, 1 expression\n",
      "Expr=ReadParquetFSSpec(b3c161e)  label\n",
      "0     6154121\n",
      "1    12187775\n",
      "Name: count, dtype: int64\n",
      "Tỷ lệ file Dask DataFrame Structure:\n",
      "               id.orig_h id.orig_p id.resp_h id.resp_p duration orig_bytes resp_bytes conn_state missed_bytes  history orig_pkts orig_ip_bytes resp_pkts resp_ip_bytes  label detailed-label proto_icmp proto_tcp proto_udp service_dhcp service_dns service_http service_irc service_ssh service_ssl\n",
      "npartitions=11                                                                                                                                                                                                                                                                                       \n",
      "                 float64   float64   float64   float64  float64    float64    float64    float64      float64  float64   float64       float64   float64       float64  int64          int64      int64     int64     int64        int64       int64        int64       int64       int64       int64\n",
      "                     ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "...                  ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "                     ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "                     ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "Dask Name: read_parquet, 1 expression\n",
      "Expr=ReadParquetFSSpec(865ff5f)  label\n",
      "0     8750899\n",
      "1    17346163\n",
      "Name: count, dtype: int64\n",
      "Tỷ lệ file Dask DataFrame Structure:\n",
      "               id.orig_h id.orig_p id.resp_h id.resp_p duration orig_bytes resp_bytes conn_state missed_bytes  history orig_pkts orig_ip_bytes resp_pkts resp_ip_bytes  label detailed-label proto_icmp proto_tcp proto_udp service_dhcp service_dns service_http service_irc service_ssh service_ssl\n",
      "npartitions=18                                                                                                                                                                                                                                                                                       \n",
      "                 float64   float64   float64   float64  float64    float64    float64    float64      float64  float64   float64       float64   float64       float64  int64          int64      int64     int64     int64        int64       int64        int64       int64       int64       int64\n",
      "                     ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "...                  ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "                     ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "                     ...       ...       ...       ...      ...        ...        ...        ...          ...      ...       ...           ...       ...           ...    ...            ...        ...       ...       ...          ...         ...          ...         ...         ...         ...\n",
      "Dask Name: read_parquet, 1 expression\n",
      "Expr=ReadParquetFSSpec(d02b013)  label\n",
      "1    26694849\n",
      "0    13491221\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#### Thay đổi hiển thị ####\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.set_option('display.max_row', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "#### Change display ####\n",
    "\n",
    "dir = [\"C:/Users/hoang/FileCSV_DACN_2025/fl_IoT23\" ,\"/mnt/c/Users/hoang/FileCSV_DACN_2025/fl_IoT23\"]\n",
    "dir = dir[0] if os.name=='nt' else dir[1]\n",
    "files = [dir +f\"/parquet_f{index}\" for index in range(3)]\n",
    "print(files)\n",
    "\n",
    "dfs = [dd.read_parquet(files[index]) for index in range(3)]\n",
    "\n",
    "for df in dfs:\n",
    "    print(f\"Tỷ lệ file {df} \", df['label'].value_counts().compute())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
