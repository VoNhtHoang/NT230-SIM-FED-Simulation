{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 08:34:12.185737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747989252.327262   15162 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747989252.367551   15162 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747989252.692965   15162 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747989252.693032   15162 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747989252.693034   15162 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747989252.693036   15162 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-23 08:34:12.733906: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/c/Users/hoang/FileCSV_DACN_2025/file1.csv', '/mnt/c/Users/hoang/FileCSV_DACN_2025/file2.csv', '/mnt/c/Users/hoang/FileCSV_DACN_2025/file3.csv']\n",
      "id.orig_h         float64\n",
      "id.orig_p         float64\n",
      "id.resp_h         float64\n",
      "id.resp_p         float64\n",
      "duration          float64\n",
      "orig_bytes        float64\n",
      "resp_bytes        float64\n",
      "conn_state        float64\n",
      "missed_bytes      float64\n",
      "history           float64\n",
      "orig_pkts         float64\n",
      "orig_ip_bytes     float64\n",
      "resp_pkts         float64\n",
      "resp_ip_bytes     float64\n",
      "detailed-label      int64\n",
      "proto_icmp          int64\n",
      "proto_tcp           int64\n",
      "proto_udp           int64\n",
      "service_dhcp        int64\n",
      "service_dns         int64\n",
      "service_http        int64\n",
      "service_irc         int64\n",
      "service_ssh         int64\n",
      "service_ssl         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import dask.dataframe as dk\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import os\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "# temp_dir = \"C:/Users/hoang/FileCSV_DACN_2025/ddos_dos_\"\n",
    "\n",
    "input_files = [f\"file{i+1}.csv\" for i in range(3)]\n",
    "temp_dir =  [\"/mnt/c/Users/hoang/FileCSV_DACN_2025/\", \"C:/Users/hoang/FileCSV_DACN_2025/\"]\n",
    "\n",
    "if os.name=='nt':\n",
    "    temp_dir=temp_dir[1]\n",
    "else:\n",
    "    temp_dir= temp_dir[0]\n",
    "\n",
    "input_files = [temp_dir + output_file for output_file in input_files]\n",
    "print(input_files)\n",
    "df = [dk.read_csv(file) for file in input_files]\n",
    "for index in range(len(df)):\n",
    "    df[index]= df[index].drop(columns='label')\n",
    "# test_df = dk.read_csv(\"FL_Dataset/test.csv\")\n",
    "\n",
    "# input_zip = \"/mnt/c/Users/hoang/FileCSV_DACN_2025/2Type.zip\"\n",
    "# csv_files = []\n",
    "# with zipfile.ZipFile(input_zip, 'r') as z:\n",
    "#     csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
    "# print(csv_files)\n",
    "# df = [dk.read_csv(f'zip://{file}::{input_zip}') for file in csv_files]\n",
    "# print(df[0].shape)\n",
    "\n",
    "print(df[1].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Len:  23\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "ratio_test_all = 0.2\n",
    "\n",
    "from dask_ml.model_selection import train_test_split \n",
    "# Split \n",
    "# train_df, val_test_df = train_test_split(df, test_size=0.20, random_state=42)\n",
    "# val_df, test_df = train_test_split(val_test_df, test_size=0.75, random_state=42)\n",
    "\n",
    "# # load từng batch\n",
    "features_len = len(df[1].columns)-1\n",
    "print(\"Feature Len: \",features_len)\n",
    "def dask_to_tf_dataset(dask_df, batch_size, num_classes): \n",
    "    def generator():\n",
    "        for batch in dask_df.to_delayed():\n",
    "            batch=batch.compute()  \n",
    "            if batch.empty:\n",
    "                continue\n",
    "\n",
    "            X = batch.drop(columns='detailed-label').values.astype(np.float32)\n",
    "            y = batch['detailed-label'].values\n",
    "            y_onehot = to_categorical(y, num_classes=num_classes)  \n",
    "\n",
    "            num_splits = max(1, len(X) // batch_size)  # Đảm bảo không chia nhỏ quá mức\n",
    "            X_batches = np.array_split(X, num_splits)\n",
    "            y_batches = np.array_split(y_onehot, num_splits)\n",
    "\n",
    "            for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "                yield X_batch, y_batch\n",
    "                \n",
    "    output_signature = ( \n",
    "        tf.TensorSpec(shape=(None, features_len), dtype=tf.float32), \n",
    "        tf.TensorSpec(shape=(None, 3), dtype=tf.int32),\n",
    "    )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(generator, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747989259.056001   15162 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2248 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# train_df1, test_df1 = df1.random_split([1 - ratio_test_all, ratio_test_all])\n",
    "# train_df2, test_df2 = df2.random_split([1 - ratio_test_all, ratio_test_all])\n",
    "# train_df3, test_df3 = df3.random_split([1 - ratio_test_all, ratio_test_all])\n",
    "batch_size = 512\n",
    "train_dfs = []\n",
    "val_dfs = []\n",
    "test_dfs= []\n",
    "for dff in df:\n",
    "    train_df, val_test_df =dff.random_split([1 - ratio_test_all, ratio_test_all])\n",
    "    test_df, val_df = val_test_df.random_split([1-0.25, 0.25])\n",
    "    train_dfs.append(train_df)\n",
    "    val_dfs.append(val_df)\n",
    "    test_dfs.append(test_df)\n",
    "   \n",
    "\n",
    "train_gens = [dask_to_tf_dataset(train_df, batch_size=batch_size,num_classes=3).repeat() for train_df in train_dfs]\n",
    "val_gens = [dask_to_tf_dataset(val_df , batch_size=batch_size, num_classes=3).repeat() for val_df in val_dfs]\n",
    "test_gens = [dask_to_tf_dataset(test_df , batch_size=batch_size, num_classes=3).repeat() for test_df in test_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22193, 44386, 66573]\n",
      "[1390, 2772, 4164]\n",
      "[4156, 8313, 12479]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tenseal as ts\n",
    "#\n",
    "from server import Server\n",
    "from client import Client\n",
    "num_servers = 1\n",
    "num_clients = 3\n",
    "\n",
    "stepsPerEpoch_Clients = [int( np.ceil(train_dfs[index].shape[0])/batch_size) for index in range(num_clients)] #[40, 40, 40] \n",
    "stepsValidate_Clients =  [int( np.ceil(val_dfs[index].shape[0])/batch_size) for index in range(num_clients)] #[5,5,5]\n",
    "stepsTest_Clients =[ int(np.ceil(test_dfs[index].shape[0])/batch_size) for index in range(num_clients)]  # [10,10,10] \n",
    "print(stepsPerEpoch_Clients)\n",
    "print(stepsValidate_Clients)\n",
    "print(stepsTest_Clients)\n",
    "# int(np.ceil(test_df.shape[0]))\n",
    "active_servers_list  = ['server_'+str(i)\\\n",
    "                        for i in range(num_servers)]\n",
    "active_clients_list  = ['client_'+str(i)\\\n",
    "                        for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['server_0']\n",
      "['client_0', 'client_1', 'client_2']\n",
      "<class 'dict'>\n",
      "Agent_Dict:  <client.Client object at 0x7fc9ff7b93a0>\n",
      "<server.Server object at 0x7fc9ff6452e0>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(active_servers_list)\n",
    "print(active_clients_list)\n",
    "\n",
    "def init_he_context():\n",
    "    \"\"\"Thiết lập context mã hóa đồng hình\"\"\"\n",
    "    context = ts.context(\n",
    "        ts.SCHEME_TYPE.CKKS, # ckks cho số thực, bfv cho int\n",
    "        poly_modulus_degree= 32768,   #8192,\n",
    "        coeff_mod_bit_sizes=[60, 40,40, 40, 60]\n",
    "    )\n",
    "    context.generate_galois_keys()\n",
    "    context.global_scale = 2**40\n",
    "    return context\n",
    "\n",
    "context = init_he_context()\n",
    "agents_dict= {}\n",
    "serverObjects={}\n",
    "clientObjects={}\n",
    "serverObjects = {server_name: Server(server_name=server_name, \\\n",
    "                        active_clients_list=active_clients_list) \\\n",
    "                        for server_name in active_servers_list}\n",
    "print(type(serverObjects))\n",
    "clientObjects = {client_name: Client(client_name, train_gens[clientID], val_gens[clientID], test_gens[clientID], \\\n",
    "                        stepsPerEpoch_Clients[clientID],stepsValidate_Clients[clientID], stepsTest_Clients[clientID],\\\n",
    "                        active_clients_list = active_clients_list, he_context=context) \\\n",
    "                        for clientID, client_name in enumerate(active_clients_list)}\n",
    "\n",
    "# for index, client_name in enumerate(active_clients_list):\n",
    "#     clientObjects[client_name].set_steps_per_epoch(stepsPerEpoch_Clients[index])\n",
    "#     clientObjects[client_name].get_steps_per_epoch()\n",
    "#     clientObjects[client_name].set_validation_steps(stepsValidate_Clients[index])\n",
    "#     clientObjects[client_name].get_validation_steps()\n",
    "#     clientObjects[client_name].set_test_steps(stepsTest_Clients)\n",
    "#     clientObjects[client_name].get_test_steps()\n",
    "    \n",
    "# lưu dict\n",
    "agents_dict['server'] = serverObjects\n",
    "agents_dict['client'] = clientObjects\n",
    "\n",
    "# init agents_dict vừa tạo vào client, server\n",
    "for agent_name, agent in serverObjects.items():\n",
    "    agent.set_agentsDict(agents_dict=agents_dict)\n",
    "for agent_name, agent in clientObjects.items():\n",
    "    agent.set_agentsDict(agents_dict=agents_dict)\n",
    "\n",
    "client_name = 'client_1'\n",
    "print(\"Agent_Dict: \", agents_dict['client'][client_name])\n",
    "\n",
    "server = agents_dict['server']['server_0']\n",
    "print(server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================== Đang chạy Iteration 1 ======================================\n",
      "Epoch 1/4\n",
      "Epoch 1/4\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747989482.750180   15254 service.cc:152] XLA service 0x7fc938008610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1747989482.751106   15254 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce GTX 1650 with Max-Q Design, Compute Capability 7.5\n",
      "2025-05-23 08:38:03.005983: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1747989483.477930   15256 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m    2/44386\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m54:18\u001b[0m 73ms/step - accuracy: 0.4045 - loss: 1.3782   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747989489.376265   15256 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  177/22193\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:28\u001b[0m 26ms/step - accuracy: 0.7453 - loss: 0.6918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 08:38:13.931273: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  181/22193\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:07\u001b[0m 28ms/step - accuracy: 0.7459 - loss: 0.6909"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 08:38:14.329078: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22434/44386\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m9:02\u001b[0m 25ms/step - accuracy: 0.8370 - loss: 0.4605"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 08:47:23.609408: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=0,k4=1,k5=1,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.7 = (f32[515,128,1,23]{3,2,1,0}, u8[0]{0}) custom-call(f32[515,128,1,23]{3,2,1,0} %bitcast.728, f32[128,128,1,3]{3,2,1,0} %bitcast.732, f32[128]{0} %bitcast.734), window={size=1x3 pad=0_0x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv1d_1_2/convolution\" source_file=\"/home/hoangvn/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-05-23 08:47:23.612269: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 3.618244755s\n",
      "Trying algorithm eng20{k2=0,k4=1,k5=1,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.7 = (f32[515,128,1,23]{3,2,1,0}, u8[0]{0}) custom-call(f32[515,128,1,23]{3,2,1,0} %bitcast.728, f32[128,128,1,3]{3,2,1,0} %bitcast.732, f32[128]{0} %bitcast.734), window={size=1x3 pad=0_0x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv1d_1_2/convolution\" source_file=\"/home/hoangvn/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22193/22193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 27ms/step - accuracy: 0.7672 - loss: 0.6153 - val_accuracy: 0.6978 - val_loss: 0.8212\n",
      "Epoch 2/4\n",
      "\u001b[1m22193/22193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 26ms/step - accuracy: 0.6971 - loss: 0.8034 - val_accuracy: 0.4124 - val_loss: 2.5930\n",
      "Epoch 3/4\n",
      "\u001b[1m44386/44386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1201s\u001b[0m 27ms/step - accuracy: 0.8078 - loss: 0.5245 - val_accuracy: 0.7539 - val_loss: 0.9314\n",
      "Epoch 2/4\n",
      "\u001b[1m22431/44386\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m8:04\u001b[0m 22ms/step - accuracy: 0.7834 - loss: 0.5690"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    server.InitLoop()\n",
    "    server.final_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14h24p__15-04/model_1.keras', '14h24p__15-04/model_2.keras', '14h24p__15-04/model_3.keras', '14h24p__15-04/model_4.keras', '14h24p__15-04/model_5.keras']\n",
      "[<Sequential name=sequential, built=True>, <Sequential name=sequential, built=True>, <Sequential name=sequential, built=True>, <Sequential name=sequential, built=True>, <Sequential name=sequential, built=True>]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# tempdirs = [\"D:/DoAnChuyenNganh_Train/client_0_log/11h18p__02-04-2025/\", \"D:/DoAnChuyenNganh_Train/client_1_log/11h18p__02-04-2025/\",  \"D:/DoAnChuyenNganh_Train/client_2_log/11h18p__02-04-2025/\"]\n",
    "timeFolder=\"14h24p__15-04/\"\n",
    "tempdirs = [f\"D:/DoAnChuyenNganh_Train/log/client_{i}_log/\" for i in range(len(active_clients_list))]\n",
    "\n",
    "\n",
    "model_names =[timeFolder+f\"model_{i+1}.keras\" for i in range(5)]\n",
    "print(model_names)\n",
    "models = {}\n",
    "\n",
    "for i, client_name in enumerate(active_clients_list):\n",
    "    models[client_name] = [load_model(tempdirs[i]+model_name) for model_name in model_names]\n",
    "print (models['client_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Batch Each Client:  [4160, 8324, 12488]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_batch_test_eachClient  =  []\n",
    "for index, test_df in enumerate(test_dfs):\n",
    "    num_samples_test = test_df.shape[0].compute()\n",
    "    # Tính số batch\n",
    "    num_batches_test = int(np.ceil(num_samples_test / batch_size))\n",
    "    num_batch_test_eachClient.append(num_batches_test)\n",
    "\n",
    "print(\"Num Batch Each Client: \", num_batch_test_eachClient)\n",
    "X_tests = {}\n",
    "Y_tests = {}\n",
    "Y_preds= {}\n",
    "for i, client_name in enumerate(active_clients_list):\n",
    "    Y_tests[client_name]={}\n",
    "    Y_preds[client_name]={}\n",
    "\n",
    "# for i, client_name in enumerate(active_clients_list):\n",
    "#     X_test = []\n",
    "#     y_test = []\n",
    "#     for X_batch, y_batch in test_gens[i].take(num_batch_test_eachClient[i]):\n",
    "#         X_test.extend(X_batch.numpy().flatten())\n",
    "#         y_test.extend(y_batch.numpy().flatten())\n",
    "#         y_pred = []\n",
    "#         for iteration in range(5):    \n",
    "#     # .as_numpy_iterator():\n",
    "#         # # take(12000):\n",
    "#         # X_test_list.append(X_batch.numpy())\n",
    "#         # y_test_list.append(y_batch.numpy())  # .numpy()\n",
    "\n",
    "#         # # Gộp tất cả batch lại\n",
    "#         # X_test = np.concatenate(X_test_list, axis=0)\n",
    "#         # y_test = np.concatenate(y_test_list, axis=0)\n",
    "\n",
    "#         # # Nếu y_test đang ở dạng one-hot, chuyển về dạng số\n",
    "#         # y_test = np.argmax(y_test, axis=1)\n",
    "#             y_pred_pre = models[client_name][iteration].predict(X_batch, verbose=0)\n",
    "#             y_pred.extend((y_pred_pre > 0.5).astype(int).flatten())\n",
    "    \n",
    "#         Y_tests[client_name][iteration] = y_test\n",
    "#         Y_preds[client_name][iteration] = y_pred\n",
    "    \n",
    "# print(Y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2136424\n",
      "<Sequential name=sequential, built=True>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 320, but received input with shape (32, 128)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 23), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Iterations)):\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(models[client_name][iteration])\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     y_pred_pre = \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     y_pred = (y_pred_pre > \u001b[32m0.5\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m).flatten()\n\u001b[32m     29\u001b[39m     precisions.append(precision_score(y_test, y_pred, average=\u001b[33m'\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec.axes.items():\n\u001b[32m    223\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m    224\u001b[39m             value,\n\u001b[32m    225\u001b[39m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    226\u001b[39m         }:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    228\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of layer \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m is \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    229\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    230\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    231\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbut received input with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    232\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    233\u001b[39m             )\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec.shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 320, but received input with shape (32, 128)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 23), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Client 0\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "Iterations = [f\"Iteration {index+1}\" for index in range(5)]\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for X_batch, y_batch in test_gens[i].take(num_batch_test_eachClient[0]):\n",
    "    X_test.append(X_batch.numpy())\n",
    "    y_test.append(y_batch.numpy())\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "\n",
    "print(len(X_test))\n",
    "for iteration in range(len(Iterations)):\n",
    "    print(models[client_name][iteration])\n",
    "    y_pred_pre = models[client_name][iteration].predict(X_test, verbose=1)\n",
    "    y_pred = (y_pred_pre > 0.5).astype(int).flatten()\n",
    "    \n",
    "    precisions.append(precision_score(y_test, y_pred, average='binary'))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='binary'))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='binary'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision Score: \", precisions)\n",
    "print(\"Recall Score: \", recalls)\n",
    "print(\"F1 Score: \", f1s)  \n",
    "\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(Iterations, precisions, marker='o', linestyle='-', label=\"Precision\", color='blue')\n",
    "plt.plot(Iterations, recalls, marker='s', linestyle='-', label=\"Recall\", color='red')\n",
    "plt.plot(Iterations, f1s, marker='^', linestyle='-', label=\"F1-score\", color='green')\n",
    "\n",
    "# Thêm tiêu đề và nhãn\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision, Recall, F1-score over different iterations _ CLIENT 0\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Giới hạn từ 0 đến 1\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị đồ thị\n",
    "plt.show()\n",
    "\n",
    "\n",
    "attack_types = ['BenignTraffic', 'DoS&DDoS']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "    # Vẽ heatmap\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=attack_types, yticklabels=attack_types)\n",
    "plt.yticks(rotation=360)\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix CLIENT 0\")\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# attack_types = ['BenignTraffic', 'DDoS-ICMP_Flood', 'DDoS-PSHACK_Flood', 'DDoS-RSTFINFlood', 'DDoS-SYN_Flood', \n",
    "#                    'DDoS-SynonymousIP_Flood', 'DDoS-TCP_Flood', 'DDoS-UDP_Flood', 'DoS-SYN_Flood', 'DoS-TCP_Flood', 'DoS-UDP_Flood']\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# Số lượng lớp (10 lớp)\n",
    "num_classes = len(attack_types)\n",
    "\n",
    "# Duyệt từng lớp để tính TP, FP, TN, FN\n",
    "for i in range(num_classes):\n",
    "    TP = cm[i, i]\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    metrics.append([attack_types[i], TP, FP, TN, FN])\n",
    "\n",
    "# Chuyển thành DataFrame\n",
    "df_metrics = pd.DataFrame(metrics, columns=[\"Attack_Types\", \"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "df_metrics.set_index(\"Attack_Types\").plot(kind=\"bar\", figsize=(12, 5), colormap=\"viridis\")\n",
    "\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"TP, FP, TN, FN for Each Class CLIENT 0\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend([\"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client 1\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "y_pred = []\n",
    "\n",
    "Iterations = [f\"Iteration {index+1}\" for index in range(5)]\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "print(num_batch_test_eachClient[1])\n",
    "\n",
    "for X_batch, y_batch in test_gens[1].take(num_batch_test_eachClient[1]):\n",
    "    X_test.append(X_batch.numpy())\n",
    "    y_test.append(y_batch.numpy())\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "\n",
    "for iteration in range(len(Iterations)):\n",
    "    y_pred_pre = models['client_1'][iteration].predict(X_test, verbose=1)\n",
    "    y_pred = (y_pred_pre > 0.5).astype(int).flatten()\n",
    "    \n",
    "    precisions.append(precision_score(y_test, y_pred, average='binary'))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='binary'))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='binary'))\n",
    "    \n",
    "print(\"Precision Score: \", precisions)\n",
    "print(\"Recall Score: \", recalls)\n",
    "print(\"F1 Score: \", f1s)  \n",
    "\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(Iterations, precisions, marker='o', linestyle='-', label=\"Precision\", color='blue')\n",
    "plt.plot(Iterations, recalls, marker='s', linestyle='-', label=\"Recall\", color='red')\n",
    "plt.plot(Iterations, f1s, marker='^', linestyle='-', label=\"F1-score\", color='green')\n",
    "\n",
    "# Thêm tiêu đề và nhãn\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision, Recall, F1-score over different iterations _ CLIENT 1\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Giới hạn từ 0 đến 1\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị đồ thị\n",
    "plt.show()\n",
    "\n",
    "\n",
    "attack_types = ['BenignTraffic', 'DoS&DDoS']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "    # Vẽ heatmap\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=attack_types, yticklabels=attack_types)\n",
    "plt.yticks(rotation=360)\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix CLIENT 1\")\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()\n",
    "\n",
    "# attack_types = ['BenignTraffic', 'DDoS-ICMP_Flood', 'DDoS-PSHACK_Flood', 'DDoS-RSTFINFlood', 'DDoS-SYN_Flood', \n",
    "#                    'DDoS-SynonymousIP_Flood', 'DDoS-TCP_Flood', 'DDoS-UDP_Flood', 'DoS-SYN_Flood', 'DoS-TCP_Flood', 'DoS-UDP_Flood']\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# Số lượng lớp (10 lớp)\n",
    "num_classes = len(attack_types)\n",
    "\n",
    "# Duyệt từng lớp để tính TP, FP, TN, FN\n",
    "for i in range(num_classes):\n",
    "    TP = cm[i, i]\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    metrics.append([attack_types[i], TP, FP, TN, FN])\n",
    "\n",
    "# Chuyển thành DataFrame\n",
    "df_metrics = pd.DataFrame(metrics, columns=[\"Attack_Types\", \"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "df_metrics.set_index(\"Attack_Types\").plot(kind=\"bar\", figsize=(12, 5), colormap=\"viridis\")\n",
    "\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"TP, FP, TN, FN for Each Class CLIENT 1\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend([\"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client 2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "y_pred = []\n",
    "Iterations = [f\"Iteration {index+1}\" for index in range(5)]\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for X_batch, y_batch in test_gens[2].take(num_batch_test_eachClient[2]):\n",
    "    X_test.append(X_batch.numpy())\n",
    "    y_test.append(y_batch.numpy())\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "\n",
    "for iteration in range(len(Iterations)):\n",
    "    y_pred_pre = models['client_2'][iteration].predict(X_test, verbose=1)\n",
    "    y_pred = (y_pred_pre > 0.5).astype(int).flatten()\n",
    "    \n",
    "    precisions.append(precision_score(y_test, y_pred, average='binary'))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='binary'))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='binary'))\n",
    "    \n",
    "print(\"Precision Score: \", precisions)\n",
    "print(\"Recall Score: \", recalls)\n",
    "print(\"F1 Score: \", f1s)  \n",
    "\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(Iterations, precisions, marker='o', linestyle='-', label=\"Precision\", color='blue')\n",
    "plt.plot(Iterations, recalls, marker='s', linestyle='-', label=\"Recall\", color='red')\n",
    "plt.plot(Iterations, f1s, marker='^', linestyle='-', label=\"F1-score\", color='green')\n",
    "\n",
    "# Thêm tiêu đề và nhãn\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision, Recall, F1-score over different iterations _ CLIENT 2\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Giới hạn từ 0 đến 1\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị đồ thị\n",
    "plt.show()\n",
    "\n",
    "\n",
    "attack_types = ['BenignTraffic', 'DoS&DDoS']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "    # Vẽ heatmap\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=attack_types, yticklabels=attack_types)\n",
    "plt.yticks(rotation=360)\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix CLIENT 2\")\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# attack_types = ['BenignTraffic', 'DDoS-ICMP_Flood', 'DDoS-PSHACK_Flood', 'DDoS-RSTFINFlood', 'DDoS-SYN_Flood', \n",
    "#                    'DDoS-SynonymousIP_Flood', 'DDoS-TCP_Flood', 'DDoS-UDP_Flood', 'DoS-SYN_Flood', 'DoS-TCP_Flood', 'DoS-UDP_Flood']\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# Số lượng lớp (10 lớp)\n",
    "num_classes = len(attack_types)\n",
    "\n",
    "# Duyệt từng lớp để tính TP, FP, TN, FN\n",
    "for i in range(num_classes):\n",
    "    TP = cm[i, i]\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    metrics.append([attack_types[i], TP, FP, TN, FN])\n",
    "\n",
    "# Chuyển thành DataFrame\n",
    "df_metrics = pd.DataFrame(metrics, columns=[\"Attack_Types\", \"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "df_metrics.set_index(\"Attack_Types\").plot(kind=\"bar\", figsize=(12, 5), colormap=\"viridis\")\n",
    "\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"TP, FP, TN, FN for Each Class CLIENT 2\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend([\"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client_0 \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_dir= \"log/client_0_log/14h24p__15-04/\"\n",
    "file_names = [f\"Iteration_{index+1}.csv\" for index in range(5)]\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy= []\n",
    "loss = []\n",
    "epochs = [\"Epoch_\"+str(i) for i in range(5)]\n",
    "for i, file_name in enumerate(file_names):\n",
    "    # Đọc file CSV\n",
    "    df = pd.read_csv(file_dir + file_name)  # Đổi tên file nếu cần\n",
    "\n",
    "    # Chuyển \"NA\" thành NaN và xử lý nếu cần\n",
    "    df.replace(\"NA\", None, inplace=True)\n",
    "\n",
    "      # Đảm bảo epoch là số nguyên\n",
    "    df[\"accuracy\"] = df[\"accuracy\"].astype(float)\n",
    "    df[\"loss\"] = df[\"loss\"].astype(float)\n",
    "    accuracy.append(df[\"accuracy\"])\n",
    "    loss.append(df[\"loss\"])\n",
    "    print(\"Iteration \"+str(i)+\": \")\n",
    "    for epoch_index,  epoch in enumerate(epochs):\n",
    "        print(epoch + f\": Accuracy: {df['accuracy'][epoch_index]} \\t Loss: {df['loss'][epoch_index]}\")\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Vẽ Accuracy\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, accuracy[i], marker=\"o\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "    \n",
    "plt.title(\"Accuracy over Epochs Client 1\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Vẽ Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, loss[i], marker=\"s\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client_1\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_dir= \"log/client_1_log/14h24p__15-04/\"\n",
    "file_names = [f\"Iteration_{index+1}.csv\" for index in range(5)]\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy= []\n",
    "loss = []\n",
    "epochs = [\"Epoch_\"+str(i) for i in range(5)]\n",
    "for i, file_name in enumerate(file_names):\n",
    "    # Đọc file CSV\n",
    "    df = pd.read_csv(file_dir + file_name)  # Đổi tên file nếu cần\n",
    "\n",
    "    # Chuyển \"NA\" thành NaN và xử lý nếu cần\n",
    "    df.replace(\"NA\", None, inplace=True)\n",
    "\n",
    "      # Đảm bảo epoch là số nguyên\n",
    "    df[\"accuracy\"] = df[\"accuracy\"].astype(float)\n",
    "    df[\"loss\"] = df[\"loss\"].astype(float)\n",
    "    accuracy.append(df[\"accuracy\"])\n",
    "    loss.append(df[\"loss\"])\n",
    "    print(\"Iteration \"+str(i)+\": \")\n",
    "    for epoch_index,  epoch in enumerate(epochs):\n",
    "        print(epoch + f\": Accuracy: {df['accuracy'][epoch_index]} \\t Loss: {df['loss'][epoch_index]}\")\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Vẽ Accuracy\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, accuracy[i], marker=\"o\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "    \n",
    "plt.title(\"Accuracy over Epochs Client 1\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Vẽ Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, loss[i], marker=\"s\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "\n",
    "plt.title(\"Loss over Epochs Client 1\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client_2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_dir= \"log/client_2_log/14h24p__15-04/\"\n",
    "file_names = [f\"Iteration_{index+1}.csv\" for index in range(5)]\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy= []\n",
    "loss = []\n",
    "epochs = [\"Epoch_\"+str(i) for i in range(5)]\n",
    "for i, file_name in enumerate(file_names):\n",
    "    # Đọc file CSV\n",
    "    df = pd.read_csv(file_dir + file_name)  # Đổi tên file nếu cần\n",
    "\n",
    "    # Chuyển \"NA\" thành NaN và xử lý nếu cần\n",
    "    df.replace(\"NA\", None, inplace=True)\n",
    "\n",
    "      # Đảm bảo epoch là số nguyên\n",
    "    df[\"accuracy\"] = df[\"accuracy\"].astype(float)\n",
    "    df[\"loss\"] = df[\"loss\"].astype(float)\n",
    "    accuracy.append(df[\"accuracy\"])\n",
    "    loss.append(df[\"loss\"])\n",
    "    print(\"Iteration \"+str(i)+\": \")\n",
    "    for epoch_index,  epoch in enumerate(epochs):\n",
    "        print(epoch + f\": Accuracy: {df['accuracy'][epoch_index]} \\t Loss: {df['loss'][epoch_index]}\")\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Vẽ Accuracy\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, accuracy[i], marker=\"o\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "    \n",
    "plt.title(\"Accuracy over Epochs _Client 2\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Vẽ Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, loss[i], marker=\"s\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "\n",
    "plt.title(\"Loss over Epochs Client 2\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
