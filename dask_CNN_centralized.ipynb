{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd3346a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id.orig_h                 float32\n",
      "id.orig_p                 float32\n",
      "id.resp_h                 float32\n",
      "id.resp_p                 float32\n",
      "duration                  float32\n",
      "orig_bytes                float32\n",
      "resp_bytes                float32\n",
      "conn_state                float32\n",
      "missed_bytes              float32\n",
      "history                   float32\n",
      "orig_pkts                 float32\n",
      "orig_ip_bytes             float32\n",
      "resp_pkts                 float32\n",
      "resp_ip_bytes             float32\n",
      "detailed-label    string[pyarrow]\n",
      "proto_icmp                  int32\n",
      "proto_tcp                   int32\n",
      "proto_udp                   int32\n",
      "service_dhcp                int32\n",
      "service_dns                 int32\n",
      "service_http                int32\n",
      "service_irc                 int32\n",
      "service_ssh                 int32\n",
      "service_ssl                 int32\n",
      "dtype: object\n",
      "        id.orig_h  id.orig_p  id.resp_h  id.resp_p  duration  orig_bytes  \\\n",
      "536603   3.232261   0.422713   2.908634   0.350872       0.0         0.0   \n",
      "536604   3.232261   0.103287   1.170498   0.350872       0.0         0.0   \n",
      "536605   3.232261   0.154855   3.229167   0.350872       0.0         0.0   \n",
      "536606   2.825261   0.313650   3.232261   0.025953       0.0         0.0   \n",
      "536607   3.227416   0.313650   3.232261   0.025953       0.0         0.0   \n",
      "\n",
      "        resp_bytes  conn_state  missed_bytes   history  ...  \\\n",
      "536603         0.0    0.562672           0.0  0.223978  ...   \n",
      "536604         0.0    0.357123           0.0  0.847846  ...   \n",
      "536605         0.0    0.357123           0.0  0.847846  ...   \n",
      "536606         0.0    0.609494           0.0  0.025953  ...   \n",
      "536607         0.0    0.609494           0.0  0.025953  ...   \n",
      "\n",
      "                   detailed-label  proto_icmp  proto_tcp  proto_udp  \\\n",
      "536603  PartOfAHorizontalPortScan           0          1          0   \n",
      "536604                          0           0          1          0   \n",
      "536605                          0           0          1          0   \n",
      "536606                          0           1          0          0   \n",
      "536607                          0           1          0          0   \n",
      "\n",
      "       service_dhcp  service_dns  service_http  service_irc  service_ssh  \\\n",
      "536603            0            0             0            0            0   \n",
      "536604            0            0             0            0            0   \n",
      "536605            0            0             0            0            0   \n",
      "536606            0            0             0            0            0   \n",
      "536607            0            0             0            0            0   \n",
      "\n",
      "        service_ssl  \n",
      "536603            0  \n",
      "536604            0  \n",
      "536605            0  \n",
      "536606            0  \n",
      "536607            0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "input_file = \"/mnt/c/Users/hoang/FileCSV_DACN_2025/iot23_cleaned.csv\"\n",
    "\n",
    "dictTypes = {}\n",
    "\n",
    "df = dd.read_csv(input_file)\n",
    "# dtype={'detailed-label': 'object'}, assume_missing=True\n",
    "\n",
    "# print(df.columns)\n",
    "for col in df.columns:\n",
    "    if col.startswith('proto') == True:\n",
    "        dictTypes[col] = 'int32'\n",
    "    elif col.startswith('service_') == True:\n",
    "        dictTypes[col] = 'int32'\n",
    "    elif col.startswith('detailed-label'):\n",
    "        dictTypes[col] = 'str'\n",
    "    else:\n",
    "        dictTypes[col]='float32'\n",
    "del df\n",
    "\n",
    "df = dd.read_csv(input_file, dtype = dictTypes)\n",
    "df = df.drop(columns=['service_0', 'label'])\n",
    "# phân loại đa nhãn, k dùng\n",
    "df = df.replace(r'[N|n][a|A][N|n]', 0)\n",
    "df = df.replace(np.nan, 0)\n",
    "print(df.dtypes)\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30a9f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['detailed-label'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc2b0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detailed-label\n",
      "Attack                                   9398\n",
      "Okiru-Attack                                3\n",
      "C&C-HeartBeat-Attack                      834\n",
      "PartOfAHorizontalPortScan-Attack            5\n",
      "FileDownload                               18\n",
      "C&C-Mirai                                   2\n",
      "DDoS                                 16108989\n",
      "C&C-FileDownload                           53\n",
      "Okiru                                60938340\n",
      "C&C-HeartBeat                           31528\n",
      "C&C-HeartBeat-FileDownload                 11\n",
      "C&C                                     17657\n",
      "PartOfAHorizontalPortScan           213583302\n",
      "C&C-Torii                                  23\n",
      "0                                    30771095\n",
      "C&C-PartOfAHorizontalPortScan             797\n",
      "Name: count, dtype: int64[pyarrow]\n"
     ]
    }
   ],
   "source": [
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ordered_labels = [0, 1, 2, 3]\n",
    "ordered_counts = [label_counts.get(label, 0) for label in ordered_labels] \n",
    "print(label_counts)\n",
    "labels = [\"Benign\",\"DDoS\",\"DoS\",\"Mirai\"]\n",
    "# Vẽ biểu đồ cột\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.bar(labels, ordered_counts, color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"Nhãn (Classes)\")\n",
    "plt.ylabel(\"Số lượng mẫu (Frequency)\")\n",
    "plt.title(\"Tỷ lệ nhãn trong Dataset\")\n",
    "plt.xticks(range(len(labels)) ,labels, rotation =45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08bd1c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoangvn/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask_expr/_collection.py:4190: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('detailed-label', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "df = df[df['detailed-label'].isin(['Okiru', 'PartOfAHorizontalPortScan', '0'])]\n",
    "\n",
    "label_map = {\n",
    "    '0': 0,\n",
    "    'PartOfAHorizontalPortScan': 1,\n",
    "    'Okiru': 2\n",
    "}\n",
    "\n",
    "df['detailed-label'] = df['detailed-label'].map(label_map).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c803cbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id.orig_h  id.orig_p  id.resp_h  id.resp_p  duration  orig_bytes  \\\n",
      "536508   3.232261   0.532167   2.468357   0.350872       0.0         0.0   \n",
      "536509   3.232261   0.677962   2.468413   0.350872       0.0         0.0   \n",
      "536510   3.232261   0.650508   2.468396   0.350872       0.0         0.0   \n",
      "536511   3.232261   0.105631   2.468380   0.350872       0.0         0.0   \n",
      "536512   3.232261   0.071794   2.468401   0.350872       0.0         0.0   \n",
      "...           ...        ...        ...        ...       ...         ...   \n",
      "536603   3.232261   0.422713   2.908634   0.350872       0.0         0.0   \n",
      "536604   3.232261   0.103287   1.170498   0.350872       0.0         0.0   \n",
      "536605   3.232261   0.154855   3.229167   0.350872       0.0         0.0   \n",
      "536606   2.825261   0.313650   3.232261   0.025953       0.0         0.0   \n",
      "536607   3.227416   0.313650   3.232261   0.025953       0.0         0.0   \n",
      "\n",
      "        resp_bytes  conn_state  missed_bytes   history  ...  detailed-label  \\\n",
      "536508         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "536509         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "536510         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "536511         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "536512         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "...            ...         ...           ...       ...  ...             ...   \n",
      "536603         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "536604         0.0    0.357123           0.0  0.847846  ...               0   \n",
      "536605         0.0    0.357123           0.0  0.847846  ...               0   \n",
      "536606         0.0    0.609494           0.0  0.025953  ...               0   \n",
      "536607         0.0    0.609494           0.0  0.025953  ...               0   \n",
      "\n",
      "        proto_icmp  proto_tcp  proto_udp  service_dhcp  service_dns  \\\n",
      "536508           0          1          0             0            0   \n",
      "536509           0          1          0             0            0   \n",
      "536510           0          1          0             0            0   \n",
      "536511           0          1          0             0            0   \n",
      "536512           0          1          0             0            0   \n",
      "...            ...        ...        ...           ...          ...   \n",
      "536603           0          1          0             0            0   \n",
      "536604           0          1          0             0            0   \n",
      "536605           0          1          0             0            0   \n",
      "536606           1          0          0             0            0   \n",
      "536607           1          0          0             0            0   \n",
      "\n",
      "        service_http  service_irc  service_ssh  service_ssl  \n",
      "536508             0            0            0            0  \n",
      "536509             0            0            0            0  \n",
      "536510             0            0            0            0  \n",
      "536511             0            0            0            0  \n",
      "536512             0            0            0            0  \n",
      "...              ...          ...          ...          ...  \n",
      "536603             0            0            0            0  \n",
      "536604             0            0            0            0  \n",
      "536605             0            0            0            0  \n",
      "536606             0            0            0            0  \n",
      "536607             0            0            0            0  \n",
      "\n",
      "[100 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.tail(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7216e8",
   "metadata": {},
   "source": [
    "# Generator + Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2222865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Global var \n",
    "batch_size = 1024\n",
    "ratio_test_all = 0.2\n",
    "\n",
    "from dask_ml.model_selection import train_test_split \n",
    "# Bước 1: Tách 80% train, 20% còn lại (val + test)\n",
    "train_df, val_test_df = train_test_split(df, test_size=0.20, random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.75, random_state=42)\n",
    "\n",
    "\n",
    "features_len = len(df.columns())-1\n",
    "def dask_to_tf_dataset(dask_df, batch_size, num_classes): \n",
    "    def generator():\n",
    "        for batch in dask_df.to_delayed():\n",
    "            batch=batch.compute()  \n",
    "            if batch.empty:\n",
    "                continue\n",
    "\n",
    "            X = batch.drop(columns='detailed-label').values.astype(np.float32)\n",
    "            y = batch['detailed-label'].values\n",
    "            # y_onehot = to_categorical(y, num_classes=num_classes)  \n",
    "\n",
    "            num_splits = max(1, len(X) // batch_size)  # Đảm bảo không chia nhỏ quá mức\n",
    "            X_batches = np.array_split(X, num_splits)\n",
    "            y_batches = np.array_split(y, num_splits)\n",
    "\n",
    "            for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "                yield X_batch, y_batch\n",
    "                \n",
    "    output_signature = ( \n",
    "        tf.TensorSpec(shape=(None, features_len), dtype=tf.float32), \n",
    "        tf.TensorSpec(shape=(None, 3), dtype=tf.int32),\n",
    "    )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(generator, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc400eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 1024\n",
    "train_gen = dask_to_tf_dataset(train_df, 1024).repeat()\n",
    "val_gen = dask_to_tf_dataset(val_df, 1024).repeat()\n",
    "test_gen = dask_to_tf_dataset(test_df, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 05:30:44.858857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746336644.991220    1312 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746336645.029427    1312 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746336645.358698    1312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746336645.358786    1312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746336645.358787    1312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746336645.358788    1312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-04 05:30:45.396280: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow is using GPU:  True\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "n_samples = df.shape[0].compute()\n",
    "steps_per_epoch = math.ceil(n_samples / batch_size)\n",
    "validation_steps = math.ceil(steps_per_epoch / 40)\n",
    "print(\"Steps/Epoch: \", steps_per_epoch)\n",
    "\n",
    "# shape\n",
    "features, labels = next(iter(train_gen))\n",
    "input_shape = (features.shape[1], 1)\n",
    "output_shape = labels.shape[1]\n",
    "\n",
    "print(f\"Input Shape: {input_shape}\")\n",
    "\n",
    "from tensorflow import keras\n",
    "# Định nghĩa mô hình CNN\n",
    "# VGG, ...\n",
    "# Conv2D, tabular, ...\n",
    "# HE, tính tương thích của HE với CNN\n",
    "# Tính chất data in, out; Học tăng cường\n",
    "start_time = datetime.now()\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Conv1D(filters=128, kernel_size=7, padding=\"same\", activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv1D(filters=128, kernel_size=7,  padding=\"same\",activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(output_shape, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     X_batch = batch[:, :-1]\n",
    "#     y_batch = batch[:, -1]\n",
    "#     y_onehot = to_categorical(y_batch, num_classes=10)\n",
    "    \n",
    "#     model.train_on_batch(X_batch, y_onehot, verbose=1)\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger(\"Centralized_Log/\"+ datetime.now().strftime(\"%Hh%Mp__%d-%m-%Y\")+\".csv\" , append=True)\n",
    "model.fit(train_gen, epochs=25,  validation_data=val_gen, \n",
    "          validation_steps=validation_steps, steps_per_epoch=steps_per_epoch, verbose = 1, callbacks=[csv_logger])\n",
    "\n",
    "\n",
    "end_time = datetime.now()\n",
    "simulated_time = end_time - start_time\n",
    "\n",
    "# Lưu mô hình\n",
    "model.save(\"Centralized_Model/cnn_model_2-0_batch512_\" + datetime.now().strftime(\"%Hh%Mp__%d-%m-%Y\")+\".keras\")\n",
    "\n",
    "print(f\"Simulated time: {simulated_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
