{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd3346a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id.orig_h                 float32\n",
      "id.orig_p                 float32\n",
      "id.resp_h                 float32\n",
      "id.resp_p                 float32\n",
      "duration                  float32\n",
      "orig_bytes                float32\n",
      "resp_bytes                float32\n",
      "conn_state                float32\n",
      "missed_bytes              float32\n",
      "history                   float32\n",
      "orig_pkts                 float32\n",
      "orig_ip_bytes             float32\n",
      "resp_pkts                 float32\n",
      "resp_ip_bytes             float32\n",
      "detailed-label    string[pyarrow]\n",
      "proto_icmp                  int32\n",
      "proto_tcp                   int32\n",
      "proto_udp                   int32\n",
      "service_dhcp                int32\n",
      "service_dns                 int32\n",
      "service_http                int32\n",
      "service_irc                 int32\n",
      "service_ssh                 int32\n",
      "service_ssl                 int32\n",
      "dtype: object\n",
      "         id.orig_h  id.orig_p  id.resp_h  id.resp_p  duration  orig_bytes  \\\n",
      "4196223   3.232261   0.422713   2.908634   0.350872       0.0         0.0   \n",
      "4196224   3.232261   0.103287   1.170498   0.350872       0.0         0.0   \n",
      "4196225   3.232261   0.154855   3.229167   0.350872       0.0         0.0   \n",
      "4196226   2.825261   0.313650   3.232261   0.025953       0.0         0.0   \n",
      "4196227   3.227416   0.313650   3.232261   0.025953       0.0         0.0   \n",
      "\n",
      "         resp_bytes  conn_state  missed_bytes   history  ...  \\\n",
      "4196223         0.0    0.562672           0.0  0.223978  ...   \n",
      "4196224         0.0    0.357123           0.0  0.847846  ...   \n",
      "4196225         0.0    0.357123           0.0  0.847846  ...   \n",
      "4196226         0.0    0.609494           0.0  0.025953  ...   \n",
      "4196227         0.0    0.609494           0.0  0.025953  ...   \n",
      "\n",
      "                    detailed-label  proto_icmp  proto_tcp  proto_udp  \\\n",
      "4196223  PartOfAHorizontalPortScan           0          1          0   \n",
      "4196224                          0           0          1          0   \n",
      "4196225                          0           0          1          0   \n",
      "4196226                          0           1          0          0   \n",
      "4196227                          0           1          0          0   \n",
      "\n",
      "        service_dhcp  service_dns  service_http  service_irc  service_ssh  \\\n",
      "4196223            0            0             0            0            0   \n",
      "4196224            0            0             0            0            0   \n",
      "4196225            0            0             0            0            0   \n",
      "4196226            0            0             0            0            0   \n",
      "4196227            0            0             0            0            0   \n",
      "\n",
      "         service_ssl  \n",
      "4196223            0  \n",
      "4196224            0  \n",
      "4196225            0  \n",
      "4196226            0  \n",
      "4196227            0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "input_file = \"/mnt/c/Users/hoang/FileCSV_DACN_2025/iot23_cleaned.csv\"\n",
    "\n",
    "dictTypes = {}\n",
    "\n",
    "df = dd.read_csv(input_file)\n",
    "# dtype={'detailed-label': 'object'}, assume_missing=True\n",
    "\n",
    "# print(df.columns)\n",
    "for col in df.columns:\n",
    "    if col.startswith('proto') == True:\n",
    "        dictTypes[col] = 'int32'\n",
    "    elif col.startswith('service_') == True:\n",
    "        dictTypes[col] = 'int32'\n",
    "    elif col.startswith('detailed-label'):\n",
    "        dictTypes[col] = 'str'\n",
    "    else:\n",
    "        dictTypes[col]='float32'\n",
    "del df\n",
    "\n",
    "df = dd.read_csv(input_file, dtype = dictTypes, blocksize=\"500MB\")\n",
    "df = df.drop(columns=['service_0', 'label'])\n",
    "# phÃ¢n loáº¡i Ä‘a nhÃ£n, k dÃ¹ng\n",
    "df = df.replace(r'[N|n][a|A][N|n]', 0)\n",
    "df = df.replace(np.nan, 0)\n",
    "print(df.dtypes)\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30a9f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['detailed-label'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc2b0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detailed-label\n",
      "Attack                                   9398\n",
      "Okiru-Attack                                3\n",
      "C&C-HeartBeat-Attack                      834\n",
      "PartOfAHorizontalPortScan-Attack            5\n",
      "FileDownload                               18\n",
      "C&C-Mirai                                   2\n",
      "DDoS                                 16108989\n",
      "C&C-FileDownload                           53\n",
      "Okiru                                60938340\n",
      "C&C-HeartBeat                           31528\n",
      "C&C-HeartBeat-FileDownload                 11\n",
      "C&C                                     17657\n",
      "PartOfAHorizontalPortScan           213583302\n",
      "C&C-Torii                                  23\n",
      "0                                    30771095\n",
      "C&C-PartOfAHorizontalPortScan             797\n",
      "Name: count, dtype: int64[pyarrow]\n"
     ]
    }
   ],
   "source": [
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ordered_labels = [0, 1, 2, 3]\n",
    "ordered_counts = [label_counts.get(label, 0) for label in ordered_labels] \n",
    "print(label_counts)\n",
    "labels = [\"Benign\",\"DDoS\",\"DoS\",\"Mirai\"]\n",
    "# Váº½ biá»ƒu Ä‘á»“ cá»™t\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.bar(labels, ordered_counts, color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"NhÃ£n (Classes)\")\n",
    "plt.ylabel(\"Sá»‘ lÆ°á»£ng máº«u (Frequency)\")\n",
    "plt.title(\"Tá»· lá»‡ nhÃ£n trong Dataset\")\n",
    "plt.xticks(range(len(labels)) ,labels, rotation =45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bd1c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoangvn/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask_expr/_collection.py:4190: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('detailed-label', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "df = df[df['detailed-label'].isin(['Okiru', 'PartOfAHorizontalPortScan', '0'])]\n",
    "\n",
    "label_map = {\n",
    "    '0': 0,\n",
    "    'PartOfAHorizontalPortScan': 1,\n",
    "    'Okiru': 2\n",
    "}\n",
    "\n",
    "df['detailed-label'] = df['detailed-label'].map(label_map).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c803cbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id.orig_h  id.orig_p  id.resp_h  id.resp_p  duration  orig_bytes  \\\n",
      "4196128   3.232261   0.532167   2.468357   0.350872       0.0         0.0   \n",
      "4196129   3.232261   0.677962   2.468413   0.350872       0.0         0.0   \n",
      "4196130   3.232261   0.650508   2.468396   0.350872       0.0         0.0   \n",
      "4196131   3.232261   0.105631   2.468380   0.350872       0.0         0.0   \n",
      "4196132   3.232261   0.071794   2.468401   0.350872       0.0         0.0   \n",
      "...            ...        ...        ...        ...       ...         ...   \n",
      "4196223   3.232261   0.422713   2.908634   0.350872       0.0         0.0   \n",
      "4196224   3.232261   0.103287   1.170498   0.350872       0.0         0.0   \n",
      "4196225   3.232261   0.154855   3.229167   0.350872       0.0         0.0   \n",
      "4196226   2.825261   0.313650   3.232261   0.025953       0.0         0.0   \n",
      "4196227   3.227416   0.313650   3.232261   0.025953       0.0         0.0   \n",
      "\n",
      "         resp_bytes  conn_state  missed_bytes   history  ...  detailed-label  \\\n",
      "4196128         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "4196129         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "4196130         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "4196131         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "4196132         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "...             ...         ...           ...       ...  ...             ...   \n",
      "4196223         0.0    0.562672           0.0  0.223978  ...               1   \n",
      "4196224         0.0    0.357123           0.0  0.847846  ...               0   \n",
      "4196225         0.0    0.357123           0.0  0.847846  ...               0   \n",
      "4196226         0.0    0.609494           0.0  0.025953  ...               0   \n",
      "4196227         0.0    0.609494           0.0  0.025953  ...               0   \n",
      "\n",
      "         proto_icmp  proto_tcp  proto_udp  service_dhcp  service_dns  \\\n",
      "4196128           0          1          0             0            0   \n",
      "4196129           0          1          0             0            0   \n",
      "4196130           0          1          0             0            0   \n",
      "4196131           0          1          0             0            0   \n",
      "4196132           0          1          0             0            0   \n",
      "...             ...        ...        ...           ...          ...   \n",
      "4196223           0          1          0             0            0   \n",
      "4196224           0          1          0             0            0   \n",
      "4196225           0          1          0             0            0   \n",
      "4196226           1          0          0             0            0   \n",
      "4196227           1          0          0             0            0   \n",
      "\n",
      "         service_http  service_irc  service_ssh  service_ssl  \n",
      "4196128             0            0            0            0  \n",
      "4196129             0            0            0            0  \n",
      "4196130             0            0            0            0  \n",
      "4196131             0            0            0            0  \n",
      "4196132             0            0            0            0  \n",
      "...               ...          ...          ...          ...  \n",
      "4196223             0            0            0            0  \n",
      "4196224             0            0            0            0  \n",
      "4196225             0            0            0            0  \n",
      "4196226             0            0            0            0  \n",
      "4196227             0            0            0            0  \n",
      "\n",
      "[100 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.tail(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7216e8",
   "metadata": {},
   "source": [
    "# Generator + Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6b9a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 17:30:45.104170: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746379845.213703   42414 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746379845.243863   42414 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746379845.491123   42414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746379845.491154   42414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746379845.491155   42414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746379845.491157   42414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-04 17:30:45.519209: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/hoangvn/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask_ml/model_selection/_split.py:464: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n",
      "/home/hoangvn/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask_ml/model_selection/_split.py:464: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow import keras\n",
    "\n",
    "#Global var \n",
    "batch_size = 512\n",
    "ratio_test_all = 0.2\n",
    "\n",
    "from dask_ml.model_selection import train_test_split \n",
    "# BÆ°á»›c 1: TÃ¡ch 80% train, 20% cÃ²n láº¡i (val + test)\n",
    "train_df, val_test_df = train_test_split(df, test_size=0.20, random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2222865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Len:  23\n"
     ]
    }
   ],
   "source": [
    "features_len = len(df.columns)-1\n",
    "print(\"Feature Len: \",features_len)\n",
    "def dask_to_tf_dataset(dask_df, batch_size, num_classes): \n",
    "    def generator():\n",
    "        for batch in dask_df.to_delayed():\n",
    "            batch=batch.compute()  \n",
    "            if batch.empty:\n",
    "                continue\n",
    "\n",
    "            X = batch.drop(columns='detailed-label').values.astype(np.float32)\n",
    "            y = batch['detailed-label'].values\n",
    "            y_onehot = to_categorical(y, num_classes=num_classes)  \n",
    "\n",
    "            num_splits = max(1, len(X) // batch_size)  # Äáº£m báº£o khÃ´ng chia nhá» quÃ¡ má»©c\n",
    "            X_batches = np.array_split(X, num_splits)\n",
    "            y_batches = np.array_split(y_onehot, num_splits)\n",
    "\n",
    "            for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "                yield X_batch, y_batch\n",
    "                \n",
    "    output_signature = ( \n",
    "        tf.TensorSpec(shape=(None, features_len), dtype=tf.float32), \n",
    "        tf.TensorSpec(shape=(None, 3), dtype=tf.int32),\n",
    "    )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(generator, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc400eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746379850.628516   42414 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2248 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "batchSize = 128\n",
    "train_gen = dask_to_tf_dataset(train_df, batchSize, 3).repeat()\n",
    "val_gen = dask_to_tf_dataset(val_df, batchSize, 3).repeat()\n",
    "test_gen = dask_to_tf_dataset(test_df, batchSize, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad31c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "n_samples = df.shape[0].compute()\n",
    "steps_per_epoch = math.ceil(n_samples / batchSize)\n",
    "validation_steps = math.ceil(steps_per_epoch / 16)\n",
    "print(\"Steps/Epoch: \", steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (23, 1)\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746368652.024788    1786 service.cc:152] XLA service 0x7f33300121b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746368652.024835    1786 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce GTX 1650 with Max-Q Design, Compute Capability 7.5\n",
      "2025-05-04 14:24:12.118388: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1746368652.488841    1786 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m     9/298138\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:11:49\u001b[0m 14ms/step - accuracy: 0.5318 - loss: 1.4260"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746368656.939913    1786 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m298138/298138\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3606s\u001b[0m 12ms/step - accuracy: 0.9988 - loss: 0.0064 - val_accuracy: 0.6944 - val_loss: 13.4587\n",
      "Epoch 2/25\n",
      "\u001b[1m298138/298138\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3522s\u001b[0m 12ms/step - accuracy: 0.9444 - loss: 0.1640 - val_accuracy: 0.9766 - val_loss: 69.7965\n",
      "Epoch 3/25\n",
      "\u001b[1m   754/298138\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m54:48\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.2613e-04"
     ]
    }
   ],
   "source": [
    "# shape\n",
    "features, labels = next(iter(train_gen))\n",
    "input_shape = (features.shape[1], 1)\n",
    "output_shape = labels.shape[1]\n",
    "\n",
    "print(f\"Input Shape: {input_shape}\")\n",
    "\n",
    "# Äá»‹nh nghÄ©a mÃ´ hÃ¬nh CNN\n",
    "# VGG, ...\n",
    "# Conv2D, tabular, ...\n",
    "# HE, tÃ­nh tÆ°Æ¡ng thÃ­ch cá»§a HE vá»›i CNN\n",
    "# TÃ­nh cháº¥t data in, out; Há»c tÄƒng cÆ°á»ng\n",
    "start_time = datetime.now()\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Conv1D(filters=128, kernel_size=7, padding=\"same\", activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv1D(filters=128, kernel_size=7,  padding=\"same\",activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(output_shape, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#sparse khi khÃ´ng onehot\n",
    "# for batch in dataloader:\n",
    "#     X_batch = batch[:, :-1]\n",
    "#     y_batch = batch[:, -1]\n",
    "#     y_onehot = to_categorical(y_batch, num_classes=10)\n",
    "    \n",
    "#     model.train_on_batch(X_batch, y_onehot, verbose=1)\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger(\"Centralized_Log/\"+ datetime.now().strftime(\"%Hh%Mp__%d-%m-%Y\")+\".csv\" , append=True)\n",
    "model.fit(train_gen, epochs=25,  validation_data=val_gen, \n",
    "          validation_steps=validation_steps, steps_per_epoch=steps_per_epoch, verbose = 1, callbacks=[csv_logger])\n",
    "\n",
    "\n",
    "end_time = datetime.now()\n",
    "simulated_time = end_time - start_time\n",
    "\n",
    "# LÆ°u mÃ´ hÃ¬nh\n",
    "model.save(\"Centralized_Model/cnn_model_2-0_batch1024_\" + datetime.now().strftime(\"%Hh%Mp__%d-%m-%Y\")+\".keras\")\n",
    "\n",
    "print(f\"Simulated time: {simulated_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01bbcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "batchSize = 512\n",
    "n_partitions = train_df.npartitions  # Tá»•ng sá»‘ chunk hiá»‡n cÃ³\n",
    "print(n_partitions)\n",
    "\n",
    "# # Táº¡o validation dataset cá»‘ Ä‘á»‹nh 1 láº§n\n",
    "# val_df_pd = val_df.compute()\n",
    "# X_val = val_df_pd.drop(columns='detailed-label').values.astype(np.float32)\n",
    "# y_val = tf.keras.utils.to_categorical(val_df_pd['detailed-label'].values, num_classes=3)\n",
    "\n",
    "# del val_df_pd\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "# del X_val\n",
    "# del y_val\n",
    "# val_dataset = val_dataset.batch(batchSize).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "def df_partition_to_dataset(dask_partition, num_classes=3, batch_size=512):\n",
    "    df = dask_partition.compute()\n",
    "    if df.empty:\n",
    "        return None\n",
    "    X = df.drop(columns='detailed-label').values.astype(np.float32)\n",
    "    y = tf.keras.utils.to_categorical(df['detailed-label'].values, num_classes)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    return ds.shuffle(len(X)).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f279fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Training on partition 1/72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746379085.743877   39304 service.cc:152] XLA service 0x7f08b4010210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746379085.743948   39304 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce GTX 1650 with Max-Q Design, Compute Capability 7.5\n",
      "2025-05-04 17:18:05.836701: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1746379086.198713   39304 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   31/20147\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:44\u001b[0m 5ms/step - accuracy: 0.8444 - loss: 0.4347"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746379089.397428   39304 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20147/20147\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 6ms/step - accuracy: 0.9902 - loss: 0.0401 - val_accuracy: 0.9927 - val_loss: 0.0716\n",
      "\n",
      "ğŸ” Training on partition 2/72\n",
      "\u001b[1m21419/21419\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.6914e-04 - val_accuracy: 1.0000 - val_loss: 1.3734e-04\n",
      "\n",
      "ğŸ” Training on partition 3/72\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(23,1)),\n",
    "    layers.Conv1D(filters=128, kernel_size=7, padding=\"same\", activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv1D(filters=128, kernel_size=7,  padding=\"same\",activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train láº§n lÆ°á»£t trÃªn tá»«ng \n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "csv_logger = CSVLogger(\"Centralized_Log/\"+ datetime.now().strftime(\"%Hh%Mp__%d-%m-%Y\")+\".csv\" , append=True)\n",
    "nepochs=25\n",
    "for epoch in range(nepochs):\n",
    "    for i in range(n_partitions):\n",
    "        print(f\"\\nğŸ” Training on partition {i+1}/{n_partitions}\")\n",
    "        train_partition = train_df.get_partition(i)\n",
    "        val_partition = val_df.get_partition(i)\n",
    "        train_ds = df_partition_to_dataset(train_partition, 3, 128)\n",
    "        val_ds = df_partition_to_dataset(val_partition, 3, 128)\n",
    "        if train_ds is not None:\n",
    "            model.fit(train_ds, epochs=1,  validation_data=val_ds, verbose = 1, callbacks=[csv_logger])\n",
    "            \n",
    "end_time = datetime.now()\n",
    "simulated_time = end_time - start_time\n",
    "\n",
    "# LÆ°u mÃ´ hÃ¬nh\n",
    "model.save(\"Centralized_Model/cnn_model_batch128_\" + datetime.now().strftime(\"%Hh%Mp__%d-%m-%Y\")+\".keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
