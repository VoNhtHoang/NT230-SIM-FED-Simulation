{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd3346a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id.orig_h         float32\n",
      "id.orig_p         float32\n",
      "id.resp_h         float32\n",
      "id.resp_p         float32\n",
      "duration          float32\n",
      "orig_bytes        float32\n",
      "resp_bytes        float32\n",
      "conn_state        float32\n",
      "missed_bytes      float32\n",
      "history           float32\n",
      "orig_pkts         float32\n",
      "orig_ip_bytes     float32\n",
      "resp_pkts         float32\n",
      "resp_ip_bytes     float32\n",
      "detailed-label      int32\n",
      "proto_icmp          int32\n",
      "proto_tcp           int32\n",
      "proto_udp           int32\n",
      "service_dhcp        int32\n",
      "service_dns         int32\n",
      "service_http        int32\n",
      "service_irc         int32\n",
      "service_ssh         int32\n",
      "service_ssl         int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#### Thay đổi hiển thị ####\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.set_option('display.max_row', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "#### Change display ####\n",
    "\n",
    "input_file = [\"/mnt/c/Users/hoang/FileCSV_DACN_2025/parquet_shuffled_IoT23\", \"C:\\\\Users\\\\hoang\\\\FileCSV_DACN_2025\\\\parquet_shuffled_IoT23\"]\n",
    "\n",
    "if os.name == 'nt':\n",
    "    input_file = input_file[1]\n",
    "else:\n",
    "    input_file = input_file[0]\n",
    "\n",
    "df = dd.read_parquet(input_file)\n",
    "# dictTypes = {}\n",
    "# df = dd.read_csv(input_file)\n",
    "# for col in df.columns:\n",
    "#     if col.startswith('proto') == True:\n",
    "#         dictTypes[col] = 'int32'\n",
    "#     elif col.startswith('service_') == True:\n",
    "#         dictTypes[col] = 'int32'\n",
    "#     # elif col == 'label':\n",
    "#     #     dictTypes[col]= 'int32'\n",
    "#     elif col.startswith('detailed-label'):\n",
    "#         dictTypes[col] = 'str'\n",
    "#     else:\n",
    "#         dictTypes[col]='float32'\n",
    "\n",
    "# df = dd.read_csv(input_file, dtype=dictTypes) # dtype = dictTypes\n",
    "df = df.drop(columns=['label'])\n",
    "\n",
    "# phân loại đa nhãn, k dùng\n",
    "# df = df.replace(r'[N|n][a|A][N|n]', 0)\n",
    "# df = df.replace(np.nan, 0)\n",
    "print(df.dtypes)\n",
    "# print(df['duration'].value_counts().compute())\n",
    "# print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a9f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['detailed-label'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc2b0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(label_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ordered_labels = [0, 1, 2, 3]\n",
    "ordered_counts = [label_counts.get(label, 0) for label in ordered_labels] \n",
    "print(label_counts)\n",
    "labels = [\"Benign\",\"DDoS\",\"DoS\",\"Mirai\"]\n",
    "# Vẽ biểu đồ cột\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.bar(labels, ordered_counts, color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"Nhãn (Classes)\")\n",
    "plt.ylabel(\"Số lượng mẫu (Frequency)\")\n",
    "plt.title(\"Tỷ lệ nhãn trong Dataset\")\n",
    "plt.xticks(range(len(labels)) ,labels, rotation =45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bd1c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoangvn/miniconda3/miniconda3/envs/doan/lib/python3.9/site-packages/dask_expr/_collection.py:4190: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('detailed-label', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "df = df[df['detailed-label'].isin(['Okiru', 'PartOfAHorizontalPortScan', '0'])]\n",
    "\n",
    "label_map = {\n",
    "    '0': 0,\n",
    "    'PartOfAHorizontalPortScan': 1,\n",
    "    'Okiru': 2\n",
    "}\n",
    "\n",
    "df['detailed-label'] = df['detailed-label'].map(label_map).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c803cbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id.orig_h         float32\n",
      "id.orig_p         float32\n",
      "id.resp_h         float32\n",
      "id.resp_p         float32\n",
      "duration          float32\n",
      "orig_bytes        float32\n",
      "resp_bytes        float32\n",
      "conn_state        float32\n",
      "missed_bytes      float32\n",
      "history           float32\n",
      "orig_pkts         float32\n",
      "orig_ip_bytes     float32\n",
      "resp_pkts         float32\n",
      "resp_ip_bytes     float32\n",
      "detailed-label      int32\n",
      "proto_icmp          int32\n",
      "proto_tcp           int32\n",
      "proto_udp           int32\n",
      "service_dhcp        int32\n",
      "service_dns         int32\n",
      "service_http        int32\n",
      "service_irc         int32\n",
      "service_ssh         int32\n",
      "service_ssl         int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7216e8",
   "metadata": {},
   "source": [
    "# Generator + Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6b9a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 10:32:18.024255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748255538.050779  101417 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748255538.058984  101417 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748255538.084728  101417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748255538.084771  101417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748255538.084774  101417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748255538.084775  101417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-26 10:32:18.093065: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dk\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow import keras\n",
    "\n",
    "#Global var \n",
    "batch_size = 512\n",
    "ratio_test_all = 0.2\n",
    "\n",
    "from dask_ml.model_selection import train_test_split \n",
    "# Bước 1: Tách 80% train, 20% còn lại (val + test)\n",
    "train_df, val_test_df = train_test_split(df, test_size=0.20, random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2222865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Len:  23\n"
     ]
    }
   ],
   "source": [
    "features_len = len(df.columns)-1\n",
    "print(\"Feature Len: \",features_len)\n",
    "def dask_to_tf_dataset(dask_df, batch_size, num_classes): \n",
    "    def generator():\n",
    "        for batch in dask_df.to_delayed():\n",
    "            batch=batch.compute()  \n",
    "            if batch.empty:\n",
    "                continue\n",
    "\n",
    "            X = batch.drop(columns='detailed-label').values.astype(np.float32)\n",
    "            y = batch['detailed-label'].values.astype(np.int32)\n",
    "            y_onehot = to_categorical(y, num_classes=num_classes) if num_classes >1 else y\n",
    "\n",
    "            num_splits = max(1, len(X) // batch_size)  # Đảm bảo không chia nhỏ quá mức\n",
    "            X_batches = np.array_split(X, num_splits)\n",
    "            y_batches = np.array_split(y_onehot, num_splits)\n",
    "\n",
    "            for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "                yield X_batch, y_batch\n",
    "                \n",
    "    output_signature = ( \n",
    "        tf.TensorSpec(shape=(None, features_len), dtype=tf.float32), \n",
    "        tf.TensorSpec(shape=(None, num_classes), dtype=tf.int32),\n",
    "    ) if num_classes>1 else ( \n",
    "        tf.TensorSpec(shape=(None, features_len), dtype=tf.float32), \n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "    )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(generator, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc400eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748255549.242137  101417 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2248 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "batchSize = 512\n",
    "value_count = len(df['detailed-label'].value_counts().compute())\n",
    "num_classes = 1 if  value_count<=2  else value_count\n",
    "train_gen = dask_to_tf_dataset(train_df, batchSize, num_classes).repeat()\n",
    "val_gen = dask_to_tf_dataset(val_df, batchSize, num_classes).repeat()\n",
    "test_gen = dask_to_tf_dataset(test_df, batchSize, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad31c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps/Epoch:  477048\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "n_samples = np.ceil(train_df.shape[0])\n",
    "steps_per_epoch = int(n_samples / (batchSize))\n",
    "validation_steps = int(steps_per_epoch / (16))\n",
    "print(\"Steps/Epoch: \", steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (23, 1) \n",
      " Output Shape: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,592</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m352\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m22,592\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,169</span> (90.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,169\u001b[0m (90.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,977</span> (89.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,977\u001b[0m (89.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Path:  Centralized_Model/cnn_model_Month05Day26__10h43p\n",
      "Epoch 1/50\n",
      "\u001b[1m47698/47704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8422 - loss: 0.3754"
     ]
    }
   ],
   "source": [
    "########### Enable XLA ###############\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "steps_per_epoch = 477048//10\n",
    "validation_steps= steps_per_epoch//16\n",
    "    \n",
    "########### Nếu không dùng XLA ###########\n",
    "# import os\n",
    "# os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_auto_jit=0\"\n",
    "\n",
    "# shape\n",
    "features, labels = next(iter(train_gen))\n",
    "input_shape = (features.shape[1], 1)\n",
    "output_shape = labels.shape[1] if num_classes > 1 else 1\n",
    "\n",
    "print(f\"Input Shape: {input_shape} \\n Output Shape: {output_shape}\")\n",
    "\n",
    "# Định nghĩa mô hình CNN\n",
    "# VGG, ...\n",
    "# Conv2D, tabular, ...\n",
    "# HE, tính tương thích của HE với CNN\n",
    "# Tính chất data in, out; Học tăng cường\n",
    "start_time = datetime.now()\n",
    "\n",
    "loss_func = 'categorical_crossentropy' if num_classes >1 else 'binary_crossentropy'\n",
    "activation_func = 'softmax' if num_classes >1 else 'sigmoid'\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    # layers.Conv1D(filters=128, kernel_size=3,  padding=\"same\",activation=\"relu\"),\n",
    "    # layers.BatchNormalization(),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1, activation= activation_func)\n",
    "])\n",
    "adam_optimizer = optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=adam_optimizer, loss=loss_func, metrics=['accuracy'])\n",
    "model.summary()\n",
    "#sparse khi không onehot\n",
    "# for batch in dataloader:\n",
    "#     X_batch = batch[:, :-1]\n",
    "#     y_batch = batch[:, -1]\n",
    "#     y_onehot = to_categorical(y_batch, num_classes=10)\n",
    "    \n",
    "#     model.train_on_batch(X_batch, y_onehot, verbose=1)\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "csv_logger = CSVLogger(\"Centralized_Log/\"+ datetime.now().strftime(\"Month%mDay%d__%Hh%Mp\")+\".csv\" , append=True)\n",
    "\n",
    "model_path =  \"Centralized_Model/cnn_model_\" + datetime.now().strftime(\"Month%mDay%d__%Hh%Mp\")\n",
    "print(\"Model Path: \", model_path)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "    ModelCheckpoint(model_path+\"_best.keras\", monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    csv_logger\n",
    "]\n",
    "model.fit(train_gen, epochs=50,  validation_data=val_gen, \n",
    "          validation_steps=validation_steps, steps_per_epoch=steps_per_epoch, verbose = 1, callbacks=callbacks)\n",
    "\n",
    "end_time = datetime.now()\n",
    "simulated_time = end_time - start_time\n",
    "\n",
    "# Lưu mô hình\n",
    "model.save(model_path+\".keras\")\n",
    "\n",
    "print(f\"Simulated time: {simulated_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01bbcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "batchSize = 512\n",
    "n_partitions = train_df.npartitions  # Tổng số chunk hiện có\n",
    "print(n_partitions)\n",
    "\n",
    "# # Tạo validation dataset cố định 1 lần\n",
    "# val_df_pd = val_df.compute()\n",
    "# X_val = val_df_pd.drop(columns='detailed-label').values.astype(np.float32)\n",
    "# y_val = tf.keras.utils.to_categorical(val_df_pd['detailed-label'].values, num_classes=3)\n",
    "\n",
    "# del val_df_pd\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "# del X_val\n",
    "# del y_val\n",
    "# val_dataset = val_dataset.batch(batchSize).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "def df_partition_to_dataset(dask_partition, num_classes=3, batch_size=512):\n",
    "    df = dask_partition.compute()\n",
    "    if df.empty:\n",
    "        return None\n",
    "    X = df.drop(columns='detailed-label').values.astype(np.float32)\n",
    "    y = tf.keras.utils.to_categorical(df['detailed-label'].values, num_classes)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    return ds.shuffle(len(X)).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Training on partition 1/72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746379085.743877   39304 service.cc:152] XLA service 0x7f08b4010210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746379085.743948   39304 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce GTX 1650 with Max-Q Design, Compute Capability 7.5\n",
      "2025-05-04 17:18:05.836701: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1746379086.198713   39304 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   31/20147\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 5ms/step - accuracy: 0.8444 - loss: 0.4347"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746379089.397428   39304 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20147/20147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 6ms/step - accuracy: 0.9902 - loss: 0.0401 - val_accuracy: 0.9927 - val_loss: 0.0716\n",
      "\n",
      "🔁 Training on partition 2/72\n",
      "\u001b[1m21419/21419\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.6914e-04 - val_accuracy: 1.0000 - val_loss: 1.3734e-04\n",
      "\n",
      "🔁 Training on partition 3/72\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(23,1)),\n",
    "    layers.Conv1D(filters=128, kernel_size=7, padding=\"same\", activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv1D(filters=128, kernel_size=7,  padding=\"same\",activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train lần lượt trên từng \n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "csv_logger = CSVLogger(\"Centralized_Log/\"+ datetime.now().strftime(\"Month %m Day %d __ %Hh%Mp\")+\".csv\" , append=True)\n",
    "nepochs=25\n",
    "for epoch in range(nepochs):\n",
    "    for i in range(n_partitions):\n",
    "        print(f\"\\n🔁 Training on partition {i+1}/{n_partitions}\")\n",
    "        train_partition = train_df.get_partition(i)\n",
    "        val_partition = val_df.get_partition(i)\n",
    "        train_ds = df_partition_to_dataset(train_partition, 3, 128)\n",
    "        val_ds = df_partition_to_dataset(val_partition, 3, 128)\n",
    "        if train_ds is not None:\n",
    "            model.fit(train_ds, epochs=1,  validation_data=val_ds, verbose = 1, callbacks=[csv_logger])\n",
    "            \n",
    "end_time = datetime.now()\n",
    "simulated_time = end_time - start_time\n",
    "\n",
    "# Lưu mô hình\n",
    "model.save(\"Centralized_Model/cnn_model_\" + datetime.now().strftime(\"Month %m Day %d __ %Hh%Mp\")+\".keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
